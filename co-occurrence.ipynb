{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 1\n",
    "\n",
    "# ========== Imports and Setup ==========\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# If you haven't downloaded stopwords before, uncomment:\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "# %pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# For nice plots in Jupyter:\n",
    "%matplotlib inline\n",
    "\n",
    "# ========== End of Imports ==========\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> ffdee8ec5d33d5ee8794f205d1cb8858e23de848
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 2\n",
    "\n",
    "# ========== 1. Load and Clean the Text File ==========\n",
    "\n",
    "# Replace 'text8.txt' with the path to your file.\n",
    "filename = \"text8.txt\"\n",
    "\n",
    "# Read file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Basic cleaning: lowercase, remove punctuation, digits, and non-ASCII chars\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)  # remove non-ASCII\n",
    "    txt = re.sub(r'\\d+', '', txt)            # remove digits\n",
    "    txt = re.sub(r'[^\\w\\s]', '', txt)        # remove punctuation\n",
    "    return txt\n",
    "\n",
    "text = clean_text(text)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# (Optional) Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [t for t in tokens if t not in stop_words and t.strip() != \"\"]\n",
    "\n",
    "print(f\"Number of tokens after cleaning and stopword removal: {len(tokens)}\")\n",
    "print(\"Sample tokens:\", tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 3\n",
    "\n",
    "# ========== 2. Build Vocabulary (Optionally Limit Size) ==========\n",
    "\n",
    "# Count frequency of all tokens\n",
    "freq = Counter(tokens)\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "most_common = freq.most_common()\n",
    "\n",
    "# Example: choose top 5000 words (adjust as needed to control memory usage)\n",
    "vocab_size = 5000\n",
    "vocab_list = [w for w, _ in most_common[:vocab_size]]\n",
    "\n",
    "# Create a mapping (word -> index)\n",
    "vocab = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"First 20 vocab words:\", vocab_list[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 4\n",
    "\n",
    "# ========== 3. Construct the Co-occurrence Matrix ==========\n",
    "\n",
    "# We will use a window_size of 2 (left+right), but you can modify.\n",
    "window_size = 2\n",
    "\n",
    "# We'll build co_occurrences dict: word -> Counter of neighbors\n",
    "co_occurrences = defaultdict(Counter)\n",
    "\n",
    "for i, word in enumerate(tokens):\n",
    "    # Skip if not in vocab\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "    \n",
    "    # We consider a window around the word\n",
    "    start = max(0, i - window_size)\n",
    "    end   = min(len(tokens), i + window_size + 1)\n",
    "    for j in range(start, end):\n",
    "        if j != i:  # don't count the word itself\n",
    "            neighbor = tokens[j]\n",
    "            if neighbor in vocab:\n",
    "                co_occurrences[word][neighbor] += 1\n",
    "\n",
    "# Initialize cooccurrence matrix (dense)\n",
    "# This can be large if vocab_size is big!\n",
    "co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "# Fill the matrix\n",
    "for w, neighbors in co_occurrences.items():\n",
    "    w_idx = vocab[w]\n",
    "    for neigh, count in neighbors.items():\n",
    "        neigh_idx = vocab[neigh]\n",
    "        co_matrix[w_idx, neigh_idx] = count\n",
    "\n",
    "print(\"Co-occurrence matrix shape:\", co_matrix.shape)\n",
    "\n",
    "# Optional: convert to a DataFrame (for debugging or inspection)\n",
    "# BUT be wary of memory usage if vocab_size is huge!\n",
    "# co_matrix_df = pd.DataFrame(co_matrix, index=vocab_list, columns=vocab_list)\n",
    "# co_matrix_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 5\n",
    "\n",
    "# ========== 4. Convert to Sparse Format ==========\n",
    "\n",
    "coo_sparse = csr_matrix(co_matrix)\n",
    "print(\"Sparse matrix shape:\", coo_sparse.shape)\n",
    "print(\"NNZ (non-zero entries):\", coo_sparse.nnz)\n",
    "\n",
    "# q : what is sparse format for my matrix here ?\n",
    "# A : Sparse matrix is a matrix in which most of the elements are zero.\n",
    "# q : why do we need to turn our matrix into sparse format ?\n",
    "# A : Sparse matrices are used in situations where the size of the matrix is very large and most of the elements are zero.\n",
    "# q : how do we populate our matrix in the first place ? why do we use np.zeros ?\n",
    "# A : We populate our matrix by iterating over the co-occurrences and filling in the counts.\n",
    "# q : why do we use np.zeros ?\n",
    "# A : We use np.zeros to initialize the matrix with zeros before filling in the counts.\n",
    "# q : what is the difference between a dense and a sparse matrix ?\n",
    "# A : A dense matrix stores all elements, while a sparse matrix only stores the non-zero elements.\n",
    "# q : so what we're doing here is initializing a matrix using np.zeros , then filling it up with our most common words and their co-occurrences, and then converting it to a sparse format ?\n",
    "# A : Yes, that's correct.\n",
    "# q : so by making it into sparse format we just removed all the pre reserved zeros ?   \n",
    "# A : Yes, that's correct. We only store the non-zero elements in the sparse format.\n",
    "# q : the csr_matrix function is used to convert a dense matrix into a sparse matrix ?\n",
    "# A : Yes, that's correct. The csr_matrix function is used to convert a dense matrix into a sparse matrix in Compressed Sparse Row format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 6\n",
    "\n",
    "# ========== 5. Dimensionality Reduction (Truncated SVD) ==========\n",
    "\n",
    "# Instead of using the raw co-occurrence matrix directly for clustering,\n",
    "# we typically project it into a lower-dimensional space using SVD.\n",
    "\n",
    "n_components = 50  # dimension of the word embeddings\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "word_embeddings = svd.fit_transform(coo_sparse)\n",
    "\n",
    "print(\"Word embeddings shape:\", word_embeddings.shape)\n",
    "print(\"Explained variance ratio (first few):\", svd.explained_variance_ratio_[:5])\n",
    "print(\"Total explained variance (50 components):\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# q : what is the purpose of dimensionality reduction ?\n",
    "# A : The purpose of dimensionality reduction is to reduce the number of features while preserving the most important information.\n",
    "# q : what is the difference between PCA and SVD ?\n",
    "# A : PCA is a statistical technique, while SVD is a matrix factorization technique. PCA uses SVD internally.\n",
    "# q : what is the difference between PCA and Truncated SVD ?\n",
    "# A : Truncated SVD is a variant of PCA that works on sparse matrices. It is more efficient than PCA for large datasets.\n",
    "# q : why do we choose 50 components ? and are the components bascaly the dimensions that we are reducing to ?\n",
    "# A : We choose 50 components as an example. The components are the dimensions of the reduced space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 7\n",
    "\n",
    "# ========== 6. K-Means Clustering and Elbow Method ==========\n",
    "\n",
    "# We'll try a range of cluster numbers (K) and plot the inertia.\n",
    "k_values = range(2, 15)\n",
    "inertias = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(word_embeddings)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertias, marker='o', linestyle='--')\n",
    "plt.title(\"Elbow Method (Co-occurrence Embeddings)\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 8\n",
    "\n",
    "# ========== 7. Silhouette Scores ==========\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(word_embeddings)\n",
    "    score = silhouette_score(word_embeddings, labels, metric='euclidean')\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, silhouette_scores, marker='o', linestyle='--', color='orange')\n",
    "plt.title(\"Silhouette Scores (Co-occurrence Embeddings)\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# You might see a peak or a plateau in the scores.\n",
    "for k, s in zip(k_values, silhouette_scores):\n",
    "    print(f\"K={k}, Silhouette={s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 9\n",
    "\n",
    "# ========== 8. Final Clustering and 2D Visualization ==========\n",
    "\n",
    "# Based on elbow or silhouette, pick a K. Let's pick 6 (example).\n",
    "final_k = 6\n",
    "kmeans = KMeans(n_clusters=final_k, random_state=42, n_init=10)\n",
    "final_labels = kmeans.fit_predict(word_embeddings)\n",
    "\n",
    "# For 2D plotting, let's reduce the embeddings from 50D to 2D.\n",
    "svd_2d = TruncatedSVD(n_components=2, random_state=42)\n",
    "word_embeddings_2d = svd_2d.fit_transform(word_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(word_embeddings_2d[:, 0],\n",
    "                      word_embeddings_2d[:, 1],\n",
    "                      c=final_labels,\n",
    "                      cmap='viridis',\n",
    "                      alpha=0.6)\n",
    "plt.title(\"Word Clusters in 2D (SVD Projection)\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Cell 10\n",
    "\n",
    "# ========== 9. Inspecting Word Clusters ==========\n",
    "\n",
    "# Reverse the 'vocab' dict to map indices -> word\n",
    "idx_to_word = {idx: w for w, idx in vocab.items()}\n",
    "\n",
    "# Collect words by cluster\n",
    "cluster_dict = defaultdict(list)\n",
    "for i, label in enumerate(final_labels):\n",
    "    cluster_dict[label].append(idx_to_word[i])\n",
    "\n",
    "# Print sample words in each cluster\n",
    "for clust_id, word_list in cluster_dict.items():\n",
    "    print(f\"--- Cluster {clust_id} ---\")\n",
    "    print(word_list[:30], \"...\\n\")  # show first 30 words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlt_cw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
