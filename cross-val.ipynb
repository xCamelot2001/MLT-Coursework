{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is NOT available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU is ready to be used: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"GPU is NOT available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.14.0\n",
      "GPUs available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3403532797827082677\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Available devices:\")\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# For inline plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after cleaning (sample):  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term \n"
     ]
    }
   ],
   "source": [
    "# Load text file\n",
    "filename = \"text8.txt\"  # Replace with your file path\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Clean text\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)  # Remove non-ASCII characters\n",
    "    txt = re.sub(r'\\d+', '', txt)            # Remove digits\n",
    "    txt = re.sub(r'[^\\w\\s]', '', txt)        # Remove punctuation\n",
    "    return txt\n",
    "\n",
    "text = clean_text(text)\n",
    "print(f\"Text after cleaning (sample): {text[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after cleaning and stopword removal: 10888361\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens = [t for t in tokens if t not in stop_words and t.strip() != \"\"]\n",
    "print(f\"Number of tokens after cleaning and stopword removal: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n",
      "Sample vocabulary: ['one', 'zero', 'nine', 'two', 'eight', 'five', 'three', 'four', 'six', 'seven']\n"
     ]
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "freq = Counter(tokens)\n",
    "most_common = freq.most_common()\n",
    "\n",
    "# Limit vocabulary size\n",
    "vocab_size = 5000\n",
    "vocab_list = [w for w, _ in most_common[:vocab_size]]\n",
    "vocab = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample vocabulary: {vocab_list[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filter tokens based on vocabulary\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m vocab_list]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_tokens, val_tokens \u001b[38;5;241m=\u001b[39m train_test_split(filtered_tokens, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Filter tokens based on vocabulary\n",
    "filtered_tokens = [t for t in tokens if t in vocab_list]\n",
    "\n",
    "# Train-test split\n",
    "train_tokens, val_tokens = train_test_split(filtered_tokens, test_size=0.2, random_state=42)\n",
    "print(f\"Training tokens: {len(train_tokens)}, Validation tokens: {len(val_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build co-occurrence matrix\n",
    "window_size = 2\n",
    "train_vocab = sorted(set(train_tokens))\n",
    "word_index = {word: idx for idx, word in enumerate(train_vocab)}\n",
    "\n",
    "co_matrix_sparse = lil_matrix((len(train_vocab), len(train_vocab)), dtype=int)\n",
    "\n",
    "for i, word in enumerate(train_tokens):\n",
    "    for j in range(max(0, i - window_size), min(len(train_tokens), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            co_matrix_sparse[word_index[word], word_index[train_tokens[j]]] += 1\n",
    "\n",
    "co_matrix_sparse = csr_matrix(co_matrix_sparse)\n",
    "print(f\"Non-zero entries in the co-occurrence matrix: {co_matrix_sparse.nnz}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and reduce dimensions\n",
    "normalized_matrix = normalize(co_matrix_sparse, norm='l2', axis=1)\n",
    "svd = TruncatedSVD(n_components=10, random_state=42)\n",
    "reduced_matrix = svd.fit_transform(normalized_matrix)\n",
    "\n",
    "print(f\"Reduced matrix shape: {reduced_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette scores\n",
    "k_values = range(2, 50)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(reduced_matrix)\n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(reduced_matrix, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"K={k}, Silhouette Score={score:.4f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Scores for Different K\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal K\n",
    "inertia_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(reduced_matrix)\n",
    "    inertia_scores.append(kmeans.inertia_)\n",
    "\n",
    "# Plot inertia scores (Elbow graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia_scores, marker='o')\n",
    "plt.title(\"Elbow Graph for K-Means\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k = k_values[np.argmin(inertia_scores)]\n",
    "print(f\"Optimal number of clusters (Elbow Method): {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation silhouette scores\n",
    "def silhouette_scores_cv(X, max_k=10, n_folds=5):\n",
    "    k_values = range(2, max_k + 1)\n",
    "    avg_scores = []\n",
    "\n",
    "    for k in k_values:\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(X[train_idx])\n",
    "            labels = kmeans.predict(X[test_idx])\n",
    "            score = silhouette_score(X[test_idx], labels, metric=\"cosine\")\n",
    "            fold_scores.append(score)\n",
    "\n",
    "        avg_scores.append(np.mean(fold_scores))\n",
    "        print(f\"K={k}, Avg Silhouette Score: {np.mean(fold_scores):.4f}\")\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "# Calculate and plot cross-validation silhouette scores\n",
    "cv_scores = silhouette_scores_cv(reduced_matrix, max_k=20)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 21), cv_scores, marker='o')\n",
    "plt.title(\"Cross-Validation Silhouette Scores\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
