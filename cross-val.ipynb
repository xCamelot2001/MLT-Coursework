{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# For nice plots in Jupyter:\n",
    "%matplotlib inline\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'text8.txt' with the path to your file.\n",
    "filename = \"text8.txt\"\n",
    "\n",
    "# Read file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Basic cleaning: lowercase, remove punctuation, digits, and non-ASCII chars\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)  # remove non-ASCII\n",
    "    txt = re.sub(r'\\d+', '', txt)            # remove digits\n",
    "    txt = re.sub(r'[^\\w\\s]', '', txt)        # remove punctuation\n",
    "    return txt\n",
    "\n",
    "text = clean_text(text)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# (Optional) Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [t for t in tokens if t not in stop_words and t.strip() != \"\"]\n",
    "\n",
    "print(f\"Number of tokens after cleaning and stopword removal: {len(tokens)}\")\n",
    "print(\"Sample tokens:\", tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of all tokens\n",
    "freq = Counter(tokens)\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "most_common = freq.most_common()\n",
    "\n",
    "# Example: choose top 5000 words (adjust as needed to control memory usage)\n",
    "vocab_size = 5000\n",
    "vocab_list = [w for w, _ in most_common[:vocab_size]]\n",
    "\n",
    "# Create a mapping (word -> index)\n",
    "vocab = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"First 20 vocab words:\", vocab_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filter tokens to include only the top 5000 most frequent words\n",
    "filtered_tokens = [t for t in tokens if t in vocab_list]\n",
    "\n",
    "# Split the filtered tokens into training and validation sets\n",
    "train_tokens, val_tokens = train_test_split(filtered_tokens, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Number of training tokens: {len(train_tokens)}\")\n",
    "print(f\"Number of validation tokens: {len(val_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed with building the co-occurrence matrix on the training data\n",
    "window_size = 2\n",
    "train_vocab = sorted(set(train_tokens))\n",
    "word_index = {word: idx for idx, word in enumerate(train_vocab)}\n",
    "\n",
    "# Proceed with the co-occurrence matrix using train_tokens\n",
    "co_matrix_sparse = lil_matrix((len(train_vocab), len(train_vocab)), dtype=int)\n",
    "\n",
    "for i, word in enumerate(train_tokens):\n",
    "    for j in range(max(0, i - window_size), min(len(train_tokens), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            co_matrix_sparse[word_index[word], word_index[train_tokens[j]]] += 1\n",
    "\n",
    "# Convert to compressed sparse row format\n",
    "co_matrix_sparse = csr_matrix(co_matrix_sparse)\n",
    "\n",
    "print(f\"Vocabulary size: {len(train_vocab)}\")\n",
    "print(f\"Non-zero entries in the co-occurrence matrix: {co_matrix_sparse.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the sparse matrix\n",
    "normalized_matrix = normalize(co_matrix_sparse, norm='l2', axis=1)\n",
    "\n",
    "# Reduce dimensionality using SVD\n",
    "svd_components = 10\n",
    "svd = TruncatedSVD(n_components=svd_components, random_state=42)\n",
    "reduced_matrix = svd.fit_transform(normalized_matrix)\n",
    "\n",
    "print(f\"Reduced matrix shape: {reduced_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 50)  # Adjust the range of K as needed\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(reduced_matrix)\n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(reduced_matrix, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"K={k}, Silhouette Score={score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Scores for Different K\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal K\n",
    "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters (K): {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_scores_cv_train_test(X, max_k=10, n_folds=10,metric='cosine'):\n",
    "    \"\"\"\n",
    "    Performs KMeans clustering for k=2..max_k with 10-fold cross-validation.\n",
    "    For each k, each fold is used as test once. We compute:\n",
    "      - Silhouette on train set\n",
    "      - Silhouette on test set\n",
    "    Then average across the 10 folds and print.\n",
    "\n",
    "    Finally, we plot the test silhouette scores by default.\n",
    "    \"\"\"\n",
    "    k_values = range(2, max_k + 1)\n",
    "    \n",
    "    avg_train_scores = []\n",
    "    avg_test_scores = []\n",
    "\n",
    "    # For each k, run KFold cross-validation\n",
    "    for k in k_values:\n",
    "        fold_train_scores = []\n",
    "        fold_test_scores  = []\n",
    "\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "\n",
    "            # Fit KMeans on train only\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(X_train)\n",
    "\n",
    "            # Predict labels\n",
    "            labels_train = kmeans.predict(X_train)\n",
    "            labels_test  = kmeans.predict(X_test)\n",
    "\n",
    "            # Compute silhouette on train\n",
    "            sil_train = silhouette_score(X_train, labels_train,metric='euclidean')\n",
    "            fold_train_scores.append(sil_train)\n",
    "\n",
    "            # Compute silhouette on test\n",
    "            sil_test = silhouette_score(X_test, labels_test,metric=metric)\n",
    "            fold_test_scores.append(sil_test)\n",
    "\n",
    "        # Average across 10 folds\n",
    "        # mean_train = np.mean(fold_train_scores)\n",
    "        mean_test  = np.mean(fold_test_scores)\n",
    "        \n",
    "        # avg_train_scores.append(mean_train)\n",
    "        avg_test_scores.append(mean_test)\n",
    "\n",
    "        print(f\"k={k}, Test Silhouette(avg)={mean_test:.4f}\")\n",
    "\n",
    "    # Plot the test scores by default\n",
    "    # plt.figure(figsize=(7,5))\n",
    "    # plt.plot(k_values, avg_test_scores, 'gx-')\n",
    "    # plt.xlabel('k (Number of Clusters)')\n",
    "    # plt.ylabel('Silhouette Score (Test Avg)')\n",
    "    # plt.title('Silhouette Scores (Train/Test CV=10) - Test Plot')\n",
    "    # plt.grid(True\n",
    "    # plt.show()\n",
    "\n",
    "    # (Optional) train vs test, :\n",
    "    # plt.figure()\n",
    "    # plt.plot(k_values, avg_train_scores, 'bo--', label='Train Score')\n",
    "    # plt.plot(k_values, avg_test_scores,  'ro-', label='Test Score')\n",
    "    # plt.xlabel('k')\n",
    "    # plt.ylabel('Silhouette')\n",
    "    # plt.title('Train vs Test Silhouette (CV=10)')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "# Usage:\n",
    "silhouette_scores_cv_train_test(X_norm, max_k=20, n_folds=10,metric='cosine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlt_cw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
