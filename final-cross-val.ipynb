{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For inline plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "filename = \"text8.txt\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Clean text\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower() # Lowercase\n",
    "    txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)  # Remove non-ASCII characters\n",
    "    txt = re.sub(r'\\d+', '', txt)            # Remove digits\n",
    "    txt = re.sub(r'[^\\w\\s]', '', txt)        # Remove punctuation\n",
    "    return txt\n",
    "\n",
    "text = clean_text(text)\n",
    "print(f\"Text after cleaning (sample): {text[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens = [t for t in tokens if t not in stop_words and t.strip() != \"\"]\n",
    "print(f\"Number of tokens after cleaning and stopword removal: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Creation\n",
    "# Count word frequencies\n",
    "# freq = Counter(tokens)\n",
    "# most_common = freq.most_common()\n",
    "\n",
    "# # Limit vocabulary size\n",
    "# vocab_size = 5000\n",
    "# vocab_list = [w for w, _ in most_common[:vocab_size]]\n",
    "# vocab = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "# Filter tokens based on vocabulary\n",
    "# filtered_tokens = [t for t in tokens if t in vocab_list]\n",
    "\n",
    "# Vocabulary Creation Using Unique Words\n",
    "vocab = set(tokens)  # All unique words\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert vocabulary to a list\n",
    "vocab_list = list(vocab)\n",
    "\n",
    "# Create a mapping from word to index and vice versa\n",
    "vocab_to_index = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "print(f\"Sample vocabulary (first 10 words): {vocab_list[:10]}\")\n",
    "\n",
    "# Filter tokens based on vocabulary\n",
    "filtered_tokens = [t for t in tokens if t in vocab]\n",
    "print(f\"Number of tokens after filtering based on vocabulary: {len(filtered_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence Matrix Construction\n",
    "# Initialize sparse co-occurrence matrix\n",
    "window_size = 2\n",
    "co_matrix_sparse = lil_matrix((vocab_size, vocab_size), dtype=int)\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "for i, word in enumerate(filtered_tokens):\n",
    "    for j in range(max(0, i - window_size), min(len(filtered_tokens), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            co_matrix_sparse[vocab_to_index[word], vocab_to_index[filtered_tokens[j]]] += 1\n",
    "\n",
    "# Convert to CSR format for efficient computation\n",
    "co_matrix_sparse = csr_matrix(co_matrix_sparse)\n",
    "\n",
    "# Check matrix properties\n",
    "print(f\"Co-occurrence matrix shape: {co_matrix_sparse.shape}\")\n",
    "print(f\"Non-zero elements in the matrix: {co_matrix_sparse.nnz}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and reduce dimensions\n",
    "co_matrix_normalized = normalize(co_matrix_sparse, norm='l2', axis=1)\n",
    "svd = TruncatedSVD(n_components=10, random_state=42)\n",
    "reduced_matrix = svd.fit_transform(co_matrix_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Set parameters for cross-validation\n",
    "max_k = 20  # Maximum number of clusters\n",
    "n_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "k_values = range(2, max_k + 1)\n",
    "\n",
    "# Initialize lists to store average scores for each metric\n",
    "avg_silhouette_scores = []\n",
    "avg_ch_scores = []\n",
    "avg_db_scores = []\n",
    "inertia_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_values:\n",
    "    silhouette_scores = []\n",
    "    ch_scores = []\n",
    "    db_scores = []\n",
    "\n",
    "    print(f\"Evaluating K={k}\")\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(reduced_matrix):\n",
    "        # Split the data into training and validation sets\n",
    "        train_data = reduced_matrix[train_idx]\n",
    "        val_data = reduced_matrix[val_idx]\n",
    "\n",
    "        # Train K-Means on the training data\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(train_data)\n",
    "\n",
    "        # Predict clusters on the validation data\n",
    "        val_labels = kmeans.predict(val_data)\n",
    "\n",
    "        # Metrics on the validation data\n",
    "        silhouette_scores.append(silhouette_score(val_data, val_labels))\n",
    "        ch_scores.append(calinski_harabasz_score(val_data, val_labels))\n",
    "        db_scores.append(davies_bouldin_score(val_data, val_labels))\n",
    "\n",
    "    # Calculate inertia for the full dataset\n",
    "    kmeans_full = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_full.fit(reduced_matrix)\n",
    "    inertia_scores.append(kmeans_full.inertia_)\n",
    "\n",
    "    # Compute average scores for this K\n",
    "    avg_silhouette_scores.append(np.mean(silhouette_scores))\n",
    "    avg_ch_scores.append(np.mean(ch_scores))\n",
    "    avg_db_scores.append(np.mean(db_scores))\n",
    "\n",
    "    print(f\"K={k}: Avg Silhouette={np.mean(silhouette_scores):.4f}, \"\n",
    "          f\"Avg CH={np.mean(ch_scores):.4f}, Avg DB={np.mean(db_scores):.4f}, \"\n",
    "          f\"Inertia={kmeans_full.inertia_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Inertia (Elbow Method)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia_scores, marker='o', label=\"Inertia\")\n",
    "plt.title(\"Elbow Method: Inertia Across K\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, avg_silhouette_scores, marker='o', label=\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Scores Across K (Cross-Validation)\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Average Silhouette Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Calinski-Harabasz Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, avg_ch_scores, marker='o', label=\"Calinski-Harabasz Score\")\n",
    "plt.title(\"Calinski-Harabasz Scores Across K (Cross-Validation)\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Average CH Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Davies-Bouldin Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, avg_db_scores, marker='o', label=\"Davies-Bouldin Score\")\n",
    "plt.title(\"Davies-Bouldin Scores Across K (Cross-Validation)\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Average DB Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = kmeans.labels_\n",
    "train_silhouette = silhouette_score(train_data, train_labels)\n",
    "train_ch_score = calinski_harabasz_score(train_data, train_labels)\n",
    "train_db_score = davies_bouldin_score(train_data, train_labels)\n",
    "\n",
    "print(f\"Training Metrics for K={k}: \"\n",
    "      f\"Silhouette={train_silhouette:.4f}, \"\n",
    "      f\"CH={train_ch_score:.4f}, \"\n",
    "      f\"DB={train_db_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "reduced_2d = pca.fit_transform(reduced_matrix)\n",
    "\n",
    "# Fit K-Means with the optimal K\n",
    "optimal_k = 5  # Adjust as needed\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans.fit(reduced_matrix)\n",
    "cluster_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse map from index to word\n",
    "index_to_word = {idx: word for word, idx in vocab_to_index.items()}\n",
    "\n",
    "# Assign each word to a cluster\n",
    "word_cluster_map = {}\n",
    "for word, idx in vocab_to_index.items():\n",
    "    cluster_id = kmeans.predict(reduced_matrix[idx].reshape(1, -1))[0]\n",
    "    word_cluster_map[word] = cluster_id\n",
    "\n",
    "# Analyze most frequent words in each cluster\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster_words = defaultdict(list)\n",
    "for word, cluster_id in word_cluster_map.items():\n",
    "    cluster_words[cluster_id].append(word)\n",
    "\n",
    "# Print top words in each cluster\n",
    "for cluster_id, words in cluster_words.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(words[:10])}\")  # Show top 10 words per cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate word clouds for each cluster\n",
    "for cluster_id, words in cluster_words.items():\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster_id}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
